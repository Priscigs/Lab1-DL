{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf25bd9-81c2-4421-87e4-52dfbf819c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, MSE Loss: 4.2899\n",
      "Epoch 100, MSE Loss: 1.2732\n",
      "Epoch 200, MSE Loss: 2.2157\n",
      "Epoch 300, MSE Loss: 4.2779\n",
      "Epoch 400, MSE Loss: 8.7906\n",
      "Epoch 500, MSE Loss: 18.6652\n",
      "Epoch 600, MSE Loss: 40.2726\n",
      "Epoch 700, MSE Loss: 87.5538\n",
      "Epoch 800, MSE Loss: 191.0141\n",
      "Epoch 900, MSE Loss: 417.4047\n",
      "Pesos aprendidos: [[19.56729586]]\n",
      "Sesgo (bias) aprendido: -68.58691183542814\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función de costo: Mean Squared Error (MSE)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    return np.sum((y_true - y_pred) ** 2) / n\n",
    "\n",
    "# Propagación hacia atrás (Backpropagation) para el Descenso de Gradiente\n",
    "def backpropagation(X, y, y_pred, learning_rate):\n",
    "    n = len(y)\n",
    "    gradient = -(2/n) * np.dot(X.T, (y - y_pred))\n",
    "    update = learning_rate * gradient\n",
    "    return update\n",
    "\n",
    "# Ejemplo de entrenamiento con regresión lineal\n",
    "def train_regression(X, y, num_epochs, learning_rate):\n",
    "    np.random.seed(42)  # Fijamos una semilla para reproducibilidad\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Inicialización de los pesos y el sesgo (bias)\n",
    "    weights = np.random.rand(num_features, 1)\n",
    "    bias = np.random.rand()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass (predicción)\n",
    "        y_pred = np.dot(X, weights) + bias\n",
    "        \n",
    "        # Cálculo del costo MSE\n",
    "        loss = mse_loss(y, y_pred)\n",
    "        \n",
    "        # Mostramos el costo cada 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, MSE Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Propagación hacia atrás y actualización de parámetros\n",
    "        weight_update = backpropagation(X, y, y_pred, learning_rate)\n",
    "        weights -= weight_update\n",
    "        bias_update = learning_rate * np.sum(2 * (y - y_pred)) / num_samples\n",
    "        bias -= bias_update\n",
    "        \n",
    "    return weights, bias\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# Supongamos que tenemos el siguiente conjunto de datos de entrenamiento\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Hiperparámetros del modelo\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Convierte y_train a forma (5, 1)\n",
    "weights, bias = train_regression(X_train, y_train_reshaped, num_epochs, learning_rate)\n",
    "\n",
    "# Mostramos los parámetros aprendidos\n",
    "print(\"Pesos aprendidos:\", weights)\n",
    "print(\"Sesgo (bias) aprendido:\", bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b2b94-dfd8-4a16-a053-787d0f265bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
